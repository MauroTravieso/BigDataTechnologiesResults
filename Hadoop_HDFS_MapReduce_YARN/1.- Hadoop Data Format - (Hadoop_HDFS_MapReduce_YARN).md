# Hadoop, HDFS, MapReduce, YARN 

### Mauro Travieso 

---

### Tasks

#### Data Format

* **What is splitability in Hadoop?**

    Splitability determines the ability to process parts of a file independently, which in turn, enables parallel processing in Hadoop; therefore, if the data is not splittable, the parallelism that allows fast queries is lost. More advanced data formats (Sequence, Avro, Parquet, ORC) offer splitability regardless of the compression codec.


* **What is a data format in Hadoop and what are the characteristics of a Data Format?**

    A file format is just a way to define how information is stored in HDFS file system. This is usually driven by the use case or the processing algorithms for aspecific domain. A data file format should be well-defined and expressive. It should be able to handle a variety of data structures specifically structs, records, maps, arrays along with strings, numbers etc. File format should be simple, binary and compressed.

    Choosing an appropriate file format can have some significant benefits: 
  
        1. Faster read times 
        2. Faster write times 
        3. Splittable files (so you don’t need to read the whole file, just a part of it) 
        4. Schema evolution support (allowing you to change the fields in a dataset) 
        5. Advanced compression support (compress the columnar files with a compression codec without sacrificing these features)
  
* **What are different data formats available in Hadoop?**
  
    ***Text input format:***

    *1) Text/CSV Files:*

    | Pros | Cons |
    | -------- | -------- |
    |* CSV file is the most commonly used data file format. | * Slow to read and write.| 
    |* It’s the most readable and also ubiquitously easy to parse.| * Slow to read and write.| 
    |* It’s the choice of format to use when export data from an RDBMS table.| * All lines in a CSV file is a record, therefore, we should not include any headers or footers.| 
    |* Text-files are inherently splittable (just split on \n characters!) | * CSV files do not support block compression, thus compressing a CSV file in Hadoop often comes at a significant read performance cost.|
    ||* CSV file has very limited support for schema evolution.|
    ||* It must be known how the file was written in order to make use of it.|

    *2) JSON Records*

    | Pros | Cons |
    | -------- | -------- |
    |* JSON is in text format that stores meta data with the data.|* Because of it’s text file, it doesn’t support block compression.|
    |* It fully supports schema evolution.|* Slow to read and write.| 
    |* You can easily add or remove attributes for each datum.|* Can’t split compressed files (Leads to Huge maps).|
    |* The file is splittable.|* Need to read/decompress all fields.| 
    ||* There is not much support in Hadoop for JSON file.|

  ***Binary input format:***

    *3) Avro Files*

    | Pros | Cons |
    | -------- | -------- |
    |* Apache Avro is a language-neutral data serialization system.|* Avro depends heavily on its schema.|
    |* It deals with data formats that can be processed by multiple languages.| * Schema is stored along with the Avro data in a file for any further processing.| 
    |* Avro is a preferred tool to serialize data in Hadoop.||
    |* The format encodes the schema of its contents directly in the file which allows you to store complex objects natively.|| 
    |* Allows schema evolutions (remove a column, add a column).||
    |* It allows every data to be written with no prior knowledge of the schema.||
    |* Supports block-level compression.||

    *4) Sequence Files*

    | Pros | Cons |
    | -------- | -------- |
    |* Binary file with a CSV-like structure.|* It does not store meta data, nor does it support full schema evolution (only schema evolution option is for appending new fields).|
    |* It does support block compression.|* Due to its unreadability, they are mostly used for intermediate data storage within a sequence of MapReduce jobs.|
    |* Records are stored in a binary format that is smaller than a text-based format would be.|* Not good for Hive, which has sql types.|
    |* The contents of the file can be compressed while also maintaining the ability to split the file into segments for multiple map tasks.|* Need to read and decompress all the fields.|
      
  ***Row-Columnar input format:***

    *5) RC (Row-Columnar) Files*
  
    | Pros | Cons |
    | -------- | -------- |
    |* Flat files consisting of binary key/value pairs, which shares much similarity with SEQUENCE FILE.|* We cannot load data into RCFILE directly.|
    |* It has significant compression and query performance benefits.|* First we need to load data into another table and then we need to overwrite it into our newly created RCFILE.|
    |* RCFILE stores columns of a table in form of record in a columnar manner.|* Depends on Meta store to supply Data types.|
    |* It first partitions rows horizontally into row splits and then it vertically partitions each row split in a columnar way.|* It doesn’t support schema evaluation and if you want to add anything to RC file you will have to rewrite the file.|
    |* RCFILE first stores the metadata of a row split, as the key part of a record, and all the data of a row split as the value part.|* It is slower to process, when read and write amplification is present.|
    |* This column oriented storage is very useful while performing analytics.| |
    
    *6) ORC (Optimized Row-Columnar) Files*
    
    | Pros | Cons |
    | -------- | -------- |
    |* ORC is the compressed version of RC file and supports all the benefits of RC file with some enhancements like ORC files compress better than RC files (reduces the size of the original data up to 75%), enabling faster queries.|* High sacrifice of cost of more memory and poor write performance.|
    |* ORC are optimized RC files that works better with Hive.|* It still does not support schema evolution.|
    |* It compresses better.|* ORC file increases CPU overhead by increasing the time it takes to decompress the relational data.|
    |* Some benchmarks indicate that ORC files compress to be the smallest of all file formats in Hadoop.|* ORC File format feature comes with the Hive 0.11 version and cannot be used with previous versions.|
    | |* It is worthwhile to note that OCR is a format primarily backed by Hortonworks, and it’s not supported by Cloudera Impala.|
    
    *7) Parquet Files*

    | Pros | Cons |
    | -------- | -------- |
    |* Datasets are partitioned both horizontally and vertically. This is particularly useful if your data processing framework just needs access to a subset of data that is stored on disk as it can access all values of a single column very quickly without reading whole records.|* Parquet format is computationally intensive on the write side.|
    |* It’s great for compression with great query performance. It supports both File-Level Compression and Block-Level Compression.|* If the application usually needs entire rows of data then the columnar formats may actually be a detriment to performance due to the increased network activity required.
    |* It’s especially efficient when querying data from specific columns.|
    |* It reduces a lot of I/O cost to make great read performance.| |
    |* It enjoys more freedom than ORC file in schema evolution, so that it can add new columns to the end of the structure.||
    |* It is also backed by Cloudera and optimized with Impala.|| 
    |* Avro and Parquet have so much in common.||
    |* These format can drastically optimize workloads, especially for Hive and Spark which tend to just read segments of records rather than the whole thing.||
    
* **What is data compression, and how does compression work in Hadoop?**

    File compression brings two major benefits: it reduces the space needed to store files, and it speeds up data transfer across the network or to or from disk. When dealing with large volumes of data, both of these savings can be significant, so it pays to carefully consider how to use compression in Hadoop.

    ***1) Compressing input files***

    If the input file is compressed, then the bytes read in from HDFS is reduced, which means less time to read data. This time conservation is beneficial to the performance of job execution.

    If the input files are compressed, they will be decompressed automatically as they are read by MapReduce, using the filename extension to determine which codec to use. For example, a file ending in .gz can be identified as gzip-compressed file and thus read with GzipCodec.

    ***2) Compressing output files***
    
    Often we need to store the output as history files. If the amount of output per day is extensive, and we often need to store history results for future use, then these accumulated results will take extensive amount of HDFS space. However, these history files may not be used very frequently, resulting in a waste of HDFS space. Therefore, it is necessary to compress the output before storing on HDFS. 

    ***3) Compressing map output***
    
    Even if your MapReduce application reads and writes uncompressed data, it may benefit from compressing the intermediate output of the map phase. Since the map output is written to disk and transferred across the network to the reducer nodes, by using a fast compressor such as LZO or Snappy, you can get performance gains simply because the volume of data to transfer is reduced.

    | Reasons to compress | Reasons not to compress |
    | -------- | -------- |
    |Data is mostly stored and not frequently processed. It is usual DWH scenario. In this case space saving can be much more significant then processing overhead.| Compressed data is not splittable. Have to be noted that many modern format are built with block level compression to enable splitting and other partial processing of the files.|
    |Compression factor is very high and thereof we save a lot of IO.|Data is created in the cluster and compression takes significant time. Have to be noted that compression usually much more CPU intensive then decompression.|
    |Decompression is very fast (like Snappy) and thereof we have a some gain with little price.|Data has little redundancy and compression gives little gain.|
    |Data already arrived compressed| |
    
* **What are the different compression formats supported in Hadoop?**

    The following list identifies some common codecs that are supported by the Hadoop framework. It is required to be sure to choose the codec that most closely matches the demands of your particular use case (for example, with workloads where the speed of processing is important, chose a codec with high decompression speeds):

    ***Gzip***: A compression utility that was adopted by the GNU project, Gzip (short for GNU zip) generates compressed files that have a .gz extension. You can use the gunzip command to decompress files that were created by a number of compression utilities, including Gzip.

    ***Bzip2:*** From a usability standpoint, Bzip2 and Gzip are similar. Bzip2 generates a better compression ratio than does Gzip, but it’s much slower. In fact, Of all the available compression codecs in Hadoop, Bzip2 is by far the slowest.

    If you’re setting up an archive that you’ll rarely need to query and space is at a high premium, then maybe would Bzip2 be worth considering.

    ***Snappy:*** The Snappy codec from Google provides modest compression ratios, but fast compression and decompression speeds. (In fact, it has the fastest decompression speeds, which makes it highly desirable for data sets that are likely to be queried often.)

    The Snappy codec is integrated into Hadoop Common, a set of common utilities that supports other Hadoop subprojects. You can use Snappy as an add-on for more recent versions of Hadoop that do not yet provide Snappy codec support.

    ***LZO:*** Similar to Snappy, LZO (short for Lempel-Ziv-Oberhumer, the trio of computer scientists who came up with the algorithm) provides modest compression ratios, but fast compression and decompression speeds. LZO is licensed under the GNU Public License (GPL).

    LZO supports splittable compression, which enables the parallel processing of compressed text file splits by your MapReduce jobs. LZO needs to create an index when it compresses a file, because with variable-length compression blocks, an index is required to tell the mapper where it can safely split the compressed file. LZO is only really desirable if you need to compress text files.

    **Guidelines for Choosing a Compression Type**

    * GZIP compression uses more CPU resources than Snappy or LZO, but provides a higher compression ratio. GZip is often a good choice for cold data, which is accessed infrequently. Snappy or LZO are a better choice for hot data, which is accessed frequently.
    
    * BZip2 can also produce more compression than GZip for some types of files, at the cost of some speed when compressing and decompressing. HBase does not support BZip2 compression.
    
    * Snappy often performs better than LZO. It is worth running tests to see if you detect a significant difference.

    * For MapReduce, if you need your compressed data to be splittable, BZip2 and LZO formats can be split. Snappy and GZip blocks are not splittable, but files with Snappy blocks inside a container file format such as SequenceFile or Avro can be split. Snappy is intended to be used with a container format, like SequenceFiles or Avro data files, rather than being used directly on plain text, for example, since the latter is not splittable and cannot be processed in parallel using MapReduce. Splittability is not relevant to HBase data.

    * For MapReduce, you can compress either the intermediate data, the output, or both. Adjust the parameters you provide for the MapReduce job accordingly. The following examples compress both the intermediate data and the output. MR2 is shown first, followed by MR1.

    With respect to the compression format usage:

    | Data Format/Compression Format | Snappy | GZip | BZip2 | LZO |
    |--------------------------------|:------:|:----:|:-----:|:---:|
    | Avro                           |    Yes |  Yes |    No |  No |
    | Parquet                        |    Yes |  Yes |    No | Yes |
    | RC                             |    Yes |  Yes |   Yes | Yes |
    | ORC                            |    Yes |  Yes |    No | Yes |
    | Text                           |    Yes |  Yes |   Yes |  No |
    | Sequence                       |    Yes |  Yes |   Yes |  No |


* **What is Schema Evolution, and how does it work with different data formats?**

    One challenge of handling big data is the frequent changing of data schema: e.g. adding/dropping columns and changing columns names. If your data schema changes a lot and you need high compatibility for your old/new applications.

    When we talk about “schema” in a database context, we are really talking about its organization—the tables, columns, views, primary keys, relationships, etc. 
    
    When we talk about schemas in the context of an individual dataset or data file, it’s helpful to simplify schema further to the individual attribute level (column headers in the simplest use case). 
    
    The schema will store the definition of each attribute and its type. Unless your data is guaranteed to never change, you’ll need to think about schema evolution, or how your data schema changes over time. How will your file format manage fields that are added or deleted?

    |Data format | Schema evolution support |
    | -------- | -------- |
    | Text/CSV Files | No, CSV file has very limited support for schema evolution. |
    | JSON Records | Yes. It fully supports schema evolution. You can easily add or remove attributes for each datum.|
    | Avro Files  | Yes. It allows schema evolutions (remove a column, add a column). |
    | Sequence Files |  No. It does not support full schema evolution (only schema evolution option is for appending new fields). |
    | RC (Row-Columnar) Files | No. It doesn’t support schema evaluation as well and if you want to add anything to RC file you will have to rewrite the file.|
    | ORC (Optimized Row-Columnar) Files | No. It still does not support schema evolution.|
    | Parquet Files| Yes. It enjoys more freedom than ORC file in schema evolution, so that it can add new columns to the end of the structure.|

* **Fill following table with yes/no in terms of splitability**

    Splittable compression is an important concept in a Hadoop context. The way Hadoop works is that files are split if they're larger than the file's block size setting, and individual file splits can be processed in parallel by different mappers.

    With respect to the splitability:

    The recent version of CDH documention fortunately states ([link](https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_data_compression_performance.html)):

    Hadoop mainly uses Snappy, GZip, BZip2, and LZO compression formats and only BZip2 that is a compression format and LZO format, which support splittable; and all other compression formats are not splittable.

    | Data Format/Compression Format | Snappy | GZip | BZip2 | LZO |
    |--------------------------------|:------:|:----:|:-----:|:---:|
    | Avro                           |    No* |   No |    No |  No |
    | Parquet                        |     No |   No |    No | Yes |
    | RC                             |     No |   No |   Yes | Yes |
    | ORC                            |     No |   No |    No | Yes |
    | Text                           |     No |   No |    No |  No |
    | Sequence                       |    No* |   No |   Yes |  No |

* **In above table, foreach Data Format, prepare a scenario which fits best**

    *For MapReduce, if you need your compressed data to be splittable, BZip2 and LZO formats can be split. Snappy and GZip blocks are not splittable; however, files with Snappy blocks inside a container file format such as SequenceFile or Avro can be split. Snappy is intended to be used with a container format, like SequenceFiles or Avro data files, rather than being used directly on plain text, for example, since the latter is not splittable and cannot be processed in parallel using MapReduce

* **Consider employee dataset, used for SQL practice, and write code in your choice of language to convert it into**

* Avro (done)
* Parquet (done)
* ORC (done)
* JSON (done)
* XML (done)
* Sequence (done)

* **And write the converted dataset to HDFS using API’s (not through commands)**

Add, if required the related .jar files
```

```

Run the script CSVformatter.scala in the scala console:
```
CSVformatter.scala

//import org.apache.log4j.{Level, Logger}
import com.databricks.spark.xml.{XmlDataFrameReader, XmlDataFrameWriter}
import org.apache.hadoop.io.{NullWritable, Text}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.{SaveMode, SparkSession}

import java.util.Properties

object CSVformatter {

  def main(args:Array[String]) {

    // Logger.getLogger("org").setLevel(Level.ERROR)

    // Spark Session
    val spark = SparkSession
      .builder()
      .appName("CSV_Formatter Avro, Parquet, JSON, XML, ORC, Sequence")
      .master("local[*]")
      .enableHiveSupport() //For ORC files
      .getOrCreate()

    spark.sparkContext.setLogLevel("ERROR")

    // Reading the CSV file
    println("****************************************************************")
    println("CSV data")
    val df = spark.read
      .format("csv")
      .option("delimiter", ",")
      .option("header", "true")
      .option("inferSchema", "true")
      .option("mode", "DROPMALFORMED")
      .option("path", "src/main/resources/employee.csv")
      .load()

    // Prints the schema
    df.printSchema()

    // For testing purposes
    df.show(5)

    // Number of rows in the file
    println("Number of TextFile(CSV) file rows: " + df.count() + "\n")


    // Reading the CSV file through MySQL
    println("****************************************************************")
    println("CSV data, extracted from the Database")

    // Jdbc Connector (Connecting Spark with MySQL)
    val jdbcDF = spark.read
      .format("jdbc")
      .option("driver","com.mysql.cj.jdbc.Driver")
      .option("url", "jdbc:mysql://localhost:3306")    //.option("url", "jdbc:mysql:dbserver")
      .option("dbtable", "employees.employee") //.option("dbtable", "schema.tablename")
      .option("user", "user")
      .option("password", "12Mbbxnq5@")
      .load()

    //    val sql="""select * from db.your_table where id>1"""
    //    val jdbcDF = spark.read
    //      .format("jdbc")
    //      .option("url", "jdbc:mysql:dbserver")
    //      .option("dbtable",  s"( $sql ) t")
    //      .option("user", "user")
    //      .option("password", "password")
    //      .load()

    // Prints the schema
    jdbcDF.printSchema()

    // For testing purposes
    jdbcDF.show(5)

    // Number of rows in the file
    println("Number of TextFile(CSV) file rows from MySQL: " + jdbcDF.count() + "\n")

    ////////////////////////////////
    // Write data to Avro format
    jdbcDF.write //df.write
      .format("com.databricks.spark.avro")
      .option("compression", "none")
      .mode(SaveMode.Overwrite)
      .save("output/avro_data/")

    // Prints the schema
    println("****************************************************************")
    println("Avro Schema")
    spark.read.format("com.databricks.spark.avro").load("output/avro_data").printSchema()

    // Show the Avro data
    println("Avro data")
    spark.read.format("com.databricks.spark.avro").load("output/avro_data")
      .show(5)

    println("Number of Avro file rows: " + spark.read.format("com.databricks.spark.avro").load("output/avro_data").count()+"\n")

    ////////////////////////////////
    // Write data to Parquet format (out-of-the-box)
    jdbcDF.write //df.write
      .option("compression","none")
      .format("parquet")
      .mode(SaveMode.Overwrite)
      .save("output/parquet_data")

    // Prints the schema
    println("****************************************************************")
    println("Parquet Schema")
    spark.read.parquet("output/parquet_data").printSchema()

    // Show the parquet data
    println("Parquet data")
    spark.read.parquet("output/parquet_data")
      .show(5)

    println("Number of Parquet file rows: " + spark.read.format("parquet").load("output/parquet_data").count()+"\n")

    //////////////////////////////
    //Write data to ORC format (out-of-the-box)
    jdbcDF.write //df.write
      .option("compression","none")
      .format("orc")
      .mode(SaveMode.Overwrite)
      .save("output/orc_data")

    // Show the ORC data
    println("****************************************************************")
    val orcDF = spark.read.orc("output/orc_data")

    // Show the ORC Schema
    println("ORC Schema")
    orcDF.printSchema()

    // Show the ORC data
    println("ORC data")
    orcDF.show(5)

    println("Number of ORC file rows: " + orcDF.count()+"\n")

    ////////////////////////////////
    // Write data to JSON format (out-of-the-box)
    jdbcDF.write //df.write
      .mode(SaveMode.Overwrite)
      .json("output/json_data")

    // Read JSON file into dataframe
    println("****************************************************************")
    println("JSON Schema")
    spark.read.json("output/json_data").printSchema()

    println("JSON data")
    spark.read.json("output/json_data").show(5)

    println("Number of JSON records: " + spark.read.json("output/json_data").count() + "\n")

    ///////////////////////////////
    // Write data to XML format
    jdbcDF.write //df.write
       // .option("compression","none") // Do not use
      .format("com.databricks.spark.xml")
      .option("rootTag","employees")
      .option("rowTag","employee")
      .mode(SaveMode.Overwrite)
      .xml("output/xml_data/")

    // Show the XML data
    println("****************************************************************")
    val xmlDF = spark.read
      .option("rowTag","employee")
      .xml("output/xml_data/")

    println("XML Schema")
    xmlDF.printSchema()

    println("XML data")
    xmlDF.show(5)
    println("Number of XML file rows: " + xmlDF.count() + "\n")

    // Close spark session
    //println("Spark Session stopped")
    //spark.close()


    ///////////////////////////////
    // Write data to Sequence format
    //val dataRDD = spark.sparkContext.sequenceFile()
    //df.write.format("sequence")

    //    val conf = new SparkConf().setAppName("SequenceFileManaging").setMaster("local[*]")
    //    val sc = new SparkContext(conf)

    val sc = spark.sparkContext
    val csvFile = sc.textFile("src/main/resources/employee.csv")
    val header = csvFile.first() // extracting the header
    csvFile.map(x => (NullWritable.get(),x))
      .filter(row => row != header)
      .saveAsSequenceFile("output/sequence_data")

    println("****************************************************************")
    println("Sequence file data")
    sc.sequenceFile("output/sequence_data",classOf[NullWritable],classOf[Text])
      .map(r => r.toString())
      //.collect() // Shows the whole data (for small datasets only)
      .take(5)
      .foreach(println)

    println("\nNumber of Sequence File Records: "+ sc.sequenceFile("output/sequence_data",classOf[NullWritable],classOf[Text]).count() + "\n")

    println("Spark Session stopped")
    sc.stop()
    //spark.close()
  }
}
```
---

